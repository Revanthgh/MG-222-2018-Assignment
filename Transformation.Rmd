---
title: "Transformation"
author: "Revanth"
date: "1 April 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the #For ROC curve and AUROC
library(pROC)

#For train-test split
library(caTools)

#For k fold cross validation
library(caret)

#For stepAIC()
library(MASS)
# k-Fold Cross Validation

# Splitting the dataset into the Training set and Test set

set.seed(123)
split = sample.split(publicc$BAD, SplitRatio = 0.75)
publicc_train = subset(publicc, split == TRUE)
publicc_test = subset(publicc, split == FALSE)

##Training set has 75% values - Test set has 25% values

# Fitting LRM to the Training set

# Full Training set model and Prediction on Test Model
logistic_model = glm(formula = BAD ~ .*.,family = binomial,data = publicc_train)

log.aic <- stepAIC(logistic_model)
summary(log.aic)

# Important Variables BAD ~ NKID + RES + NETIN + AES:NETIN + AES:NETOUT + RES:AGE + AGE:NETIN + AGE:NETOUT

full_pred = predict(log.aic, type = 'response', newdata = publicc_test[-6])
plot.roc(publicc_test$BAD,full_pred,print.auc = TRUE)

#AUC - 51.7%


# Applying k-Fold Cross Validation


folds = createFolds(publicc_train$BAD, k = 10)

par(mfrow=c(4,4))

cross_validate = lapply(folds, function(x) {
  
  training_fold = publicc_train[-x, ]
  
  test_fold = publicc_train[x, ]
  
  logistic_model = glm(formula = BAD ~ NKID + RES + NETIN + AES:NETIN + AES:NETOUT + RES:AGE + AGE:NETIN + AGE:NETOUT,family = binomial,data = training_fold)
  
  ##Removing BAD from test data
  prob_pred = predict(logistic_model, type = 'response', newdata = test_fold[-6])
  
  plot.roc(test_fold$BAD,prob_pred,print.auc = TRUE)
  
  #cat(auc(test_fold$BAD,prob_pred))
  
  y_pred = ifelse(prob_pred < 0.5, 0, 1)
  
  conf_matrix = table(test_fold[, 6], y_pred)
  
  #accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  
  #return(accuracy)
  return(conf_matrix)
})


comb_conf_matrix <- cross_validate


accuracy = 0
TotSens = 0
TotSpec = 0
for(folds in comb_conf_matrix){
  accuracy = accuracy + (folds[1]+folds[4])/(folds[1]+folds[2]+folds[3]+folds[4])*100
  TotSens = TotSens + folds[4]/(folds[2]+folds[4])*100
  TotSpec = TotSpec + folds[1]/(folds[3]+folds[1])*100
}

cat("Mean Accuracy :",accuracy/10,"\n Mean Sensitivity :", TotSens/10, "\nMean Specificity :", TotSpec/10)
cat("False Positive Rate :", 100 - TotSpec/10, "\nFalse Negative Rate :", 100 - TotSens/10)

#For automatic binning
sapply(c('dplyr', 'car', 'caret', 'e1071', 'knitr', 'reshape2', 'corrplot','rpart', 
         'scales', 'shiny', 'survival', 'gridExtra', 'devtools', 'pec', 'MASS', 'pROC', 
         'manipulate'), 
       install.packages)

sapply(c('dplyr', 'car', 'caret', 'e1071', 'knitr', 'reshape2', 'corrplot','rpart', 
         'scales', 'survival', 'gridExtra', 'devtools', 'pec', 'MASS', 'pROC', 
         'manipulate'), 
       require, character.only = TRUE)

devtools::install_github('JianhuaHuang/streamlineR')
library(streamlineR)



setwd("C:\\Users\\SONY VAIO\\Desktop\\Advanced_Assignment_2")

# read data set
publicc = read.csv('public.csv', header=TRUE, stringsAsFactors=TRUE)


#Preprocessing data
##Create AGE variable from DOB
##Age = 0 implies age is unknown
publicc$AGE <- 99-publicc$DOB

#Remove DOB column
publicc$DOB <- NULL

##Convert PHON,NKID,DEP to Categorical
publicc$PHON <- as.factor(publicc$PHON)
publicc$NKID <- as.factor(publicc$NKID)
publicc$DEP <- as.factor(publicc$DEP)

View(publicc)

##Binning the Numerical values
##Binning takes place based on Recursive Partitioning Tress - this function checks if there is a split on the
##data if we form a tree based on just BAD and the variable. If a split is formed, bins are created
##according to the splits
lg.bin.age <- bin.rpart(formula = BAD ~ AGE, data = publicc, 
                        rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.age)


lg.bin.sinc <- bin.rpart(formula = BAD ~ SINC, data = publicc, 
                         rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.sinc)


lg.bin.dainc <- bin.rpart(formula = BAD ~ DAINC, data = publicc, 
                          rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.dainc)

lg.bin.dhval <- bin.rpart(formula = BAD ~ DHVAL, data = publicc, 
                          rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.dhval)

lg.bin.dmort <- bin.rpart(formula = BAD ~ DMORT, data = publicc, 
                          rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.dmort)

lg.bin.doutm <- bin.rpart(formula = BAD ~ DOUTM, data = publicc, 
                          rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.doutm)

lg.bin.doutl <- bin.rpart(formula = BAD ~ DOUTL, data = publicc, 
                          rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.doutl)

lg.bin.douthp <- bin.rpart(formula = BAD ~ DOUTHP, data = publicc, 
                           rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.douthp)

lg.bin.doutcc <- bin.rpart(formula = BAD ~ DOUTCC, data = publicc, 
                           rcontrol = rpart.control(minbucket = .01 * nrow(publicc)))

str(lg.bin.doutcc)

## We can see that binning has occured only for the following variables :
#  AGE, DAINC, DOUTCC, DOUTM

##Replacing the variables with their corresponding Bins in the Data
publicc$AGE <- lg.bin.age$bins
publicc$DAINC <- lg.bin.dainc$bins
publicc$DOUTM <- lg.bin.doutm$bins
publicc$DOUTCC <- lg.bin.doutcc$bins

##Visualizing the Weight of Evidence for the Categorical Variables - only those with Bins
col.x <- c('AGE', 'DAINC', 'DOUTM', 'DOUTCC', 'PHON', 'AES', 'RES', 'NKID', 'DEP')
stat.train <- level.stat(publicc, x = col.x, y = 'BAD', flag.0 = 0, flag.1 = 1)
View(stat.train$Variable.IV)

ggstat(data = stat.train, var = "Variable.IV", x = "Group", y = "Rate.1", 
       y.label = "Perc.1", y.label.col = "red", y.title = NULL, 
       bar.col = "cornflowerblue", width = "Rate.group", width.label = "Perc.group", 
       width.label.col = "black", ncol = NULL, theme = "classic", 
       background = "white")

ggstat(stat.train, y = 'WOE', y.label = 'WOE.round', width = .2, 
       width.label = NULL, ncol = 4)


# k-Fold Cross Validation

# Splitting the dataset into the Training set and Test set

set.seed(123)
split = sample.split(publicc$BAD, SplitRatio = 0.75)
publicc_train = subset(publicc, split == TRUE)
publicc_test = subset(publicc, split == FALSE)
View(publicc)

##Training set has 75% values - Test set has 25% values

# Fitting LRM to the Training set

# Full Training set model and Prediction on Test Model
logistic_model = glm(formula = BAD ~ . ,family = binomial,data = publicc_train)
summary(logistic_model)

logistic_model_2 = glm(formula = BAD ~ (AES+DAINC+AGE)*(AES+DAINC+AGE) ,family = binomial,data = publicc_train)
summary(logistic_model_2)


log.aic <- stepAIC(logistic_model_2)
summary(log.aic)

# Important Variables BAD ~ DAINC + AGE

full_pred = predict(log.aic, type = 'response', newdata = publicc_test[-14])
plot.roc(publicc_test$BAD,full_pred,print.auc = TRUE)

#AUC - 64%

# Applying k-Fold Cross Validation


folds = createFolds(publicc_train$BAD, k = 10)

par(mfrow=c(4,4))

cross_validate = lapply(folds, function(x) {
  
  training_fold = publicc_train[-x, ]
  
  test_fold = publicc_train[x, ]
  
  logistic_model = glm(formula = BAD ~ DAINC + AGE,family = binomial,data = training_fold)
  
  ##Removing BAD from test data
  prob_pred = predict(logistic_model, type = 'response', newdata = test_fold[-14])
  
  plot.roc(test_fold$BAD,prob_pred,print.auc = TRUE)
  
  cat(auc(test_fold$BAD,prob_pred),"+")
  
  y_pred = ifelse(prob_pred < 0.5, 0, 1)
  
  conf_matrix = table(test_fold[, 14], y_pred)
  
  #accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  
  #return(accuracy)
  return(conf_matrix)
})


comb_conf_matrix <- cross_validate


accuracy = 0
TotSens = 0
TotSpec = 0
for(folds in comb_conf_matrix){
  accuracy = accuracy + (folds[1]+folds[4])/(folds[1]+folds[2]+folds[3]+folds[4])*100
  TotSens = TotSens + folds[4]/(folds[2]+folds[4])*100
  TotSpec = TotSpec + folds[1]/(folds[3]+folds[1])*100
}

cat("Mean Accuracy :",accuracy/10,"\n Mean Sensitivity :", TotSens/10, "\nMean Specificity :", TotSpec/10)
cat("False Positive Rate :", 100 - TotSpec/10, "\nFalse Negative Rate :", 100 - TotSens/10)

cat("\nMean AUC :",60.048)
##Replacing the variables with their corresponding Bins in the Data
publicc$AGE <- lg.bin.age$bins
publicc$DAINC <- lg.bin.dainc$bins
publicc$DOUTM <- lg.bin.doutm$bins
publicc$DOUTCC <- lg.bin.doutcc$bins

##Visualizing the Weight of Evidence for the Categorical Variables - only those with Bins
col.x <- c('AGE', 'DAINC', 'DOUTM', 'DOUTCC', 'PHON', 'AES', 'RES', 'NKID', 'DEP')
stat.train <- level.stat(publicc, x = col.x, y = 'BAD', flag.0 = 0, flag.1 = 1)
View(stat.train$Variable.IV)

ggstat(data = stat.train, var = "Variable.IV", x = "Group", y = "Rate.1", 
       y.label = "Perc.1", y.label.col = "red", y.title = NULL, 
       bar.col = "cornflowerblue", width = "Rate.group", width.label = "Perc.group", 
       width.label.col = "black", ncol = NULL, theme = "classic", 
       background = "white")

ggstat(stat.train, y = 'WOE', y.label = 'WOE.round', width = .2, 
       width.label = NULL, ncol = 4)


# k-Fold Cross Validation

# Splitting the dataset into the Training set and Test set

set.seed(123)
split = sample.split(publicc$BAD, SplitRatio = 0.75)
publicc_train = subset(publicc, split == TRUE)
publicc_test = subset(publicc, split == FALSE)
View(publicc)

##Training set has 75% values - Test set has 25% values

# Fitting LRM to the Training set

# Full Training set model and Prediction on Test Model
logistic_model = glm(formula = BAD ~ . ,family = binomial,data = publicc_train)
summary(logistic_model)

logistic_model_2 = glm(formula = BAD ~ (AES+DAINC+AGE)*(AES+DAINC+AGE) ,family = binomial,data = publicc_train)
summary(logistic_model_2)


log.aic <- stepAIC(logistic_model_2)
summary(log.aic)

# Important Variables BAD ~ DAINC + AGE

full_pred = predict(log.aic, type = 'response', newdata = publicc_test[-14])
plot.roc(publicc_test$BAD,full_pred,print.auc = TRUE)

#AUC - 64%

# Applying k-Fold Cross Validation


folds = createFolds(publicc_train$BAD, k = 10)

par(mfrow=c(4,4))

cross_validate = lapply(folds, function(x) {
  
  training_fold = publicc_train[-x, ]
  
  test_fold = publicc_train[x, ]
  
  logistic_model = glm(formula = BAD ~ DAINC + AGE,family = binomial,data = training_fold)
  
  ##Removing BAD from test data
  prob_pred = predict(logistic_model, type = 'response', newdata = test_fold[-14])
  
  plot.roc(test_fold$BAD,prob_pred,print.auc = TRUE)
  
  cat(auc(test_fold$BAD,prob_pred),"+")
  
  y_pred = ifelse(prob_pred < 0.5, 0, 1)
  
  conf_matrix = table(test_fold[, 14], y_pred)
  
  #accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  
  #return(accuracy)
  return(conf_matrix)
})


comb_conf_matrix <- cross_validate


accuracy = 0
TotSens = 0
TotSpec = 0
for(folds in comb_conf_matrix){
  accuracy = accuracy + (folds[1]+folds[4])/(folds[1]+folds[2]+folds[3]+folds[4])*100
  TotSens = TotSens + folds[4]/(folds[2]+folds[4])*100
  TotSpec = TotSpec + folds[1]/(folds[3]+folds[1])*100
}

cat("Mean Accuracy :",accuracy/10,"\n Mean Sensitivity :", TotSens/10, "\nMean Specificity :", TotSpec/10)
cat("False Positive Rate :", 100 - TotSpec/10, "\nFalse Negative Rate :", 100 - TotSens/10)

cat("\nMean AUC :",60.048)





